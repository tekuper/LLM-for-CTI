{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm9Z41q0r4b-",
        "outputId": "7b702439-a480-45b4-ec25-119d27d41ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Error fetching https://research.checkpoint.com/2024/sharp-dragon-expands-towards-africa-and-the-caribbean/: 403 Client Error: Forbidden for url: https://research.checkpoint.com/2024/sharp-dragon-expands-towards-africa-and-the-caribbean/\n",
            "Error fetching https://research.checkpoint.com/2023/blindeagle-targeting-ecuador-with-sharpened-tools/: 403 Client Error: Forbidden for url: https://research.checkpoint.com/2023/blindeagle-targeting-ecuador-with-sharpened-tools/\n",
            "Error fetching https://www.bleepingcomputer.com/news/security/apt37-hackers-deploy-new-fadestealer-eavesdropping-malware/: 403 Client Error: Forbidden for url: https://www.bleepingcomputer.com/news/security/apt37-hackers-deploy-new-fadestealer-eavesdropping-malware/\n",
            "Error fetching https://www.uscloud.com/blog/hackers-target-government-defense-contractors-with-new-backdoor-malware/: 403 Client Error: Forbidden for url: https://www.uscloud.com/blog/hackers-target-government-defense-contractors-with-new-backdoor-malware/\n",
            "Data collection completed. Scraped data saved to /content/drive/My Drive/scraped_data_full.csv\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Step 3: Expanded list of URLs to scrape (you can add more URLs if needed)\n",
        "urls = [\n",
        "    \"https://www.proofpoint.com/us/blog/threat-insight/bumblebee-buzzes-back-black\",\n",
        "    \"https://www.seqrite.com/blog/sidecopys-multi-platform-onslaught-leveraging-winrar-zero-day-and-linux-variant-of-ares-rat/\",\n",
        "    \"https://csirt-cti.net/2024/01/23/stately-taurus-targets-myanmar/\",\n",
        "    \"https://unit42.paloaltonetworks.com/stately-taurus-attacks-se-asian-government/\",\n",
        "    \"https://medium.com/@zyadlzyatsoc/comprehensive-analysis-of-emotet-malware-part-1-by-zyad-elzyat-35d5cf33a3c0\",\n",
        "    \"https://www.fortinet.com/blog/threat-research/bandook-persistent-threat-that-keeps-evolving\",\n",
        "    \"https://research.checkpoint.com/2024/sharp-dragon-expands-towards-africa-and-the-caribbean/\",\n",
        "    \"https://www.microsoft.com/en-us/security/blog/2024/05/15/threat-actors-misusing-quick-assist-in-social-engineering-attacks-leading-to-ransomware/\",\n",
        "    \"https://blogs.blackberry.com/en/2023/02/blind-eagle-apt-c-36-targets-colombia\",\n",
        "    \"https://research.checkpoint.com/2023/blindeagle-targeting-ecuador-with-sharpened-tools/\",\n",
        "    \"https://cloud.google.com/blog/topics/threat-intelligence/turla-galaxy-opportunity/\",\n",
        "    \"https://www.telsy.com/en/turla-venomous-bear-updates-its-arsenal-newpass-appears-on-the-apt-threat-scene/\",\n",
        "    # More URLs...\n",
        "    \"https://www.bleepingcomputer.com/news/security/apt37-hackers-deploy-new-fadestealer-eavesdropping-malware/\",\n",
        "    \"https://www.uscloud.com/blog/hackers-target-government-defense-contractors-with-new-backdoor-malware/\",\n",
        "    \"https://www.mandiant.com/resources/blog/apt29-wineloader-german-political-parties\",\n",
        "    \"https://www.cyberark.com/resources/blog/apt29s-attack-on-microsoft-tracking-cozy-bears-footprints/\"\n",
        "    # Add more URLs here if needed...\n",
        "]\n",
        "\n",
        "# Step 4: Function to filter out irrelevant content (such as \"Sign up\", \"LinkedIn\", etc.)\n",
        "def filter_text(text):\n",
        "    # Filter out common irrelevant patterns like \"Sign up\", \"Login\", etc.\n",
        "    unwanted_phrases = ['Sign up', 'Login', 'LinkedIn', 'Home', 'All rights reserved', 'Read more']\n",
        "    for phrase in unwanted_phrases:\n",
        "        text = text.replace(phrase, \"\")\n",
        "    return text\n",
        "\n",
        "# Step 5: Function to scrape text content from each URL\n",
        "def scrape_content(url):\n",
        "    try:\n",
        "        # Send a GET request to fetch the webpage\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for failed requests\n",
        "\n",
        "        # Parse the webpage using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract text from <p> (paragraphs) and <span> elements\n",
        "        paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
        "        spans = [span.get_text(strip=True) for span in soup.find_all('span')]\n",
        "\n",
        "        # Apply text filtering to clean up irrelevant content\n",
        "        filtered_paragraphs = [filter_text(p) for p in paragraphs]\n",
        "        filtered_spans = [filter_text(span) for span in spans]\n",
        "\n",
        "        # Extract general body text and filter it\n",
        "        body_text = filter_text(soup.get_text(strip=True))\n",
        "\n",
        "        return {\n",
        "            \"URL\": url,\n",
        "            \"Paragraphs\": \" \".join(filtered_paragraphs),\n",
        "            \"Spans\": \" \".join(filtered_spans),\n",
        "            \"Text\": body_text\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return {\n",
        "            \"URL\": url,\n",
        "            \"Paragraphs\": \"\",\n",
        "            \"Spans\": \"\",\n",
        "            \"Text\": \"\"\n",
        "        }\n",
        "\n",
        "# Step 6: Scrape data from all URLs and save to a CSV\n",
        "def collect_and_save_data(urls, output_file):\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = [\"URL\", \"Paragraphs\", \"Spans\", \"Text\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Loop through each URL and scrape the content\n",
        "        for url in urls:\n",
        "            content = scrape_content(url)\n",
        "            writer.writerow(content)\n",
        "\n",
        "# Step 7: Specify the output CSV file location in Google Drive\n",
        "output_file = '/content/drive/My Drive/scraped_data_full.csv'  # Change this to your preferred path\n",
        "\n",
        "# Step 8: Call the function to collect data and save it\n",
        "collect_and_save_data(urls, output_file)\n",
        "\n",
        "print(f\"Data collection completed. Scraped data saved to {output_file}\")\n"
      ]
    }
  ]
}